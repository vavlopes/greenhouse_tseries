Meio_poroso = 0
# )
)
##########################lista criada para agregar o cenario 1, nos 45 pontos,
####separados
u = list()
for(i in 1:45){
pos_temp = grep(paste0("temp",i,"$"),colnames(dat_new))
pos_ur = grep(paste0("ur",i,"$"),colnames(dat_new))
dat_cenario_1= dat_new[,c(1:4,pos_temp,pos_ur,95:104)]
dat_cenario_1 = cbind(dat_cenario_1,dat_config)
dat_cenario_1 = dat_cenario_1[,-grep("x1",colnames(dat_cenario_1))]
u = c(u, list(dat_cenario_1))
u[[i]]=as.data.frame(u[[i]])
u[[i]] = u[[i]][,-c(0,1)]
write.csv(u[[i]],paste0("../data/Dados_cenarios/",i,"_Cenario1.csv"),
row.names = FALSE,
quote = FALSE)
}
#############inserindo coordenadas###################
xyz = data.frame(p = 0:20, x = NA , y = NA , z = NA)
xyz$x = 1.6 + 1.6*floor(xyz$p /7)
xyz$y = 2.3 + (xyz$p %% 7)*2.3
coord = rbind(xyz,xyz)
coord$z = rep(c(1.2,2.4),each=21)
xyz_add = data.frame(p = NA, x=c(3.2),y=c(2*2.3,4*2.3,6*2.3),z=4.5)
coord = rbind(coord, xyz_add)[,-1]
# estabelencendo o centro geometrico dos ponto no ponto 32 (centro da GH)
coord$x = coord$x - 3.2
coord$y = coord$y - 9.2
coord$z = coord$z - 2.4
for(i in 1:45){
u[[i]]$x = coord[i,1]
u[[i]]$y = coord[i,2]
u[[i]]$z = coord[i,3]
}
for(i in 1:45){
names(u[[i]])[4]<-paste("temp")
names(u[[i]])[5]<-paste("ur")
}
dados_completos = u[[1]]
for(dy in u[-1]){
dados_completos = rbind(dados_completos, dy)
}
#datas estruturadas de forma a ficar ordenada corretamente
dados_completos$data = as.Date(dados_completos$data, "%d/%m/%Y")
dados_ordenados = dados_completos[order(dados_completos$hora),]
dados_ordenados = dados_ordenados[order(dados_ordenados$data),]
dados_ordenados$cenario = "Cenario_1"
dados_ordenados$range_datas = paste0(range(dados_ordenados$data)[1]," | ",
range(dados_ordenados$data)[2])
write.csv(dados_ordenados,paste0("../data/dat_step_10_",
unique(dados_ordenados$cenario),".csv"))
library(tidyverse)
library(gtools) #for ordering names
rm(list = ls())
gc()
#Verifying all files to be modified
files_tbmod = list.files('../../data_raw/',
full.names = T)
# Reading_data ------------------------------------------------------------
#reading and storing all files from files_tbmod
#cleaning columns and creatig concat coordinates for future filtering
df_list = lapply(files_tbmod,
function(x) read_delim(x, delim = ",",col_names = TRUE) %>%
select(-X1) %>%
select(-(Lateral_plastico:Meio_poroso)) %>%
mutate(concat_coord = paste0(as.character(x),as.character(y),as.character(z))) %>%
as.data.frame())
# creating look back for time series prediction
look_back = function(df, colNames, lags = 1:6){ #data until one hour back
n = nrow(df)
vet_names = colNames
for(lag in lags){
for(col in vet_names){
new_col = paste0(col,lag)
row_range_in = 1:(n-lag)
df[,new_col] = c(rep(NA,lag), unname(df[row_range_in,col]))
}
}
return(df)
}
#Function to apply look back
apply_lback = function(df){
#criando campo concatenado para filtro
unique_coords = unique(df$concat_coord)
dfi = list()
#defining columns to be lagged
col_lagged = df %>%
select(-c(data,hora,medicao, x, y, z, cenario, concat_coord)) %>%
colnames()
col_lagged = c("temp","ur")
# for filtering and lagging for each coordinate
for(xyz in unique_coords){
dfi[[xyz]] = df %>% filter(concat_coord == xyz)
dfi[[xyz]] = look_back(dfi[[xyz]], col_lagged)
}
df = do.call(bind_rows, dfi)
sortedNames = mixedsort(colnames(df))
df = df[c(sortedNames)]
save(df, file = paste0("../../data/df",min(df$data),".RData"))
return(df)
}
a = sapply(df_list, function(x) apply_lback(x))
setwd("C:/Users/vinic/Google Drive/Mestrado/pratical_project/variability_part2/greenhouse_tseries/code")
library(tidyverse)
library(gtools) #for ordering names
rm(list = ls())
gc()
#Verifying all files to be modified
files_tbmod = list.files('../../data_raw/',
full.names = T)
# Reading_data ------------------------------------------------------------
#reading and storing all files from files_tbmod
#cleaning columns and creatig concat coordinates for future filtering
df_list = lapply(files_tbmod,
function(x) read_delim(x, delim = ",",col_names = TRUE) %>%
select(-X1) %>%
select(-(Lateral_plastico:Meio_poroso)) %>%
mutate(concat_coord = paste0(as.character(x),as.character(y),as.character(z))) %>%
as.data.frame())
# creating look back for time series prediction
look_back = function(df, colNames, lags = 1:6){ #data until one hour back
n = nrow(df)
vet_names = colNames
for(lag in lags){
for(col in vet_names){
new_col = paste0(col,lag)
row_range_in = 1:(n-lag)
df[,new_col] = c(rep(NA,lag), unname(df[row_range_in,col]))
}
}
return(df)
}
#Function to apply look back
apply_lback = function(df){
#criando campo concatenado para filtro
unique_coords = unique(df$concat_coord)
dfi = list()
#defining columns to be lagged
col_lagged = df %>%
select(-c(data,hora,medicao, x, y, z, cenario, concat_coord)) %>%
colnames()
col_lagged = c("temp","ur")
# for filtering and lagging for each coordinate
for(xyz in unique_coords){
dfi[[xyz]] = df %>% filter(concat_coord == xyz)
dfi[[xyz]] = look_back(dfi[[xyz]], col_lagged)
}
df = do.call(bind_rows, dfi)
sortedNames = mixedsort(colnames(df))
df = df[c(sortedNames)]
save(df, file = paste0("../../data/df",min(df$data),".RData"))
return(df)
}
a = sapply(df_list, function(x) apply_lback(x))
head(a[[1]])
head(a)
a = lapply(df_list, function(x) apply_lback(x))
a[[1]]
library(tidyverse)
library(gtools) #for ordering names
rm(list = ls())
gc()
#Verifying all files to be modified
files_tbmod = list.files('../../data_raw/',
full.names = T)
# Reading_data ------------------------------------------------------------
#reading and storing all files from files_tbmod
#cleaning columns and creatig concat coordinates for future filtering
df_list = lapply(files_tbmod,
function(x) read_delim(x, delim = ",",col_names = TRUE) %>%
select(-X1) %>%
select(-(Lateral_plastico:Meio_poroso)) %>%
mutate(concat_coord = paste0(as.character(x),as.character(y),as.character(z))) %>%
as.data.frame())
# creating look back for time series prediction
look_back = function(df, colNames, lags = 1:6){ #data until one hour back
n = nrow(df)
vet_names = colNames
for(lag in lags){
for(col in vet_names){
new_col = paste0(col,lag)
row_range_in = 1:(n-lag)
df[,new_col] = c(rep(NA,lag), unname(df[row_range_in,col]))
}
}
return(df)
}
#Function to apply look back
apply_lback = function(df){
#criando campo concatenado para filtro
unique_coords = unique(df$concat_coord)
dfi = list()
#defining columns to be lagged
col_lagged = df %>%
select(-c(data,hora,medicao, x, y, z, cenario, concat_coord)) %>%
colnames()
# for filtering and lagging for each coordinate
for(xyz in unique_coords){
dfi[[xyz]] = df %>% filter(concat_coord == xyz)
dfi[[xyz]] = look_back(dfi[[xyz]], col_lagged)
}
df = do.call(bind_rows, dfi)
sortedNames = mixedsort(colnames(df))
df = df[c(sortedNames)]
save(df, file = paste0("../../data/df",min(df$data),".RData"))
return(df)
}
a = lapply(df_list, function(x) apply_lback(x))
a[[1]]
library(tidyverse)
library(gtools) #for ordering names
rm(list = ls())
gc()
#Verifying all files to be modified
files_tbmod = list.files('../../data_raw/',
full.names = T)
# Reading_data ------------------------------------------------------------
#reading and storing all files from files_tbmod
#cleaning columns and creatig concat coordinates for future filtering
df_list = lapply(files_tbmod,
function(x) read_delim(x, delim = ",",col_names = TRUE) %>%
select(-X1) %>%
select(-(Lateral_plastico:Meio_poroso)) %>%
mutate(concat_coord = paste0(as.character(x),as.character(y),as.character(z))) %>%
as.data.frame())
# creating look back for time series prediction
look_back = function(df, colNames, lags = 1:6){ #data until one hour back
n = nrow(df)
vet_names = colNames
for(lag in lags){
for(col in vet_names){
new_col = paste0("prev_",lag,"_",col)
row_range_in = 1:(n-lag)
df[,new_col] = c(rep(NA,lag), unname(df[row_range_in,col]))
}
}
return(df)
}
#Function to apply look back
apply_lback = function(df){
#criando campo concatenado para filtro
unique_coords = unique(df$concat_coord)
dfi = list()
#defining columns to be lagged
col_lagged = df %>%
select(-c(data,hora,medicao, x, y, z, cenario, concat_coord)) %>%
colnames()
# for filtering and lagging for each coordinate
for(xyz in unique_coords){
dfi[[xyz]] = df %>% filter(concat_coord == xyz)
dfi[[xyz]] = look_back(dfi[[xyz]], col_lagged)
}
df = do.call(bind_rows, dfi)
sortedNames = mixedsort(colnames(df))
df = df[c(sortedNames)]
save(df, file = paste0("../../data/df",min(df$data),".RData"))
return(df)
}
a = lapply(df_list, function(x) apply_lback(x))
head(a[[2]])
library(tidyverse)
library(gtools) #for ordering names
rm(list = ls())
gc()
#Verifying all files to be modified
files_tbmod = list.files('../../data_raw/',
full.names = T)
# Reading_data ------------------------------------------------------------
#reading and storing all files from files_tbmod
#cleaning columns and creatig concat coordinates for future filtering
df_list = lapply(files_tbmod,
function(x) read_delim(x, delim = ",",col_names = TRUE) %>%
select(-X1) %>%
select(-(Lateral_plastico:Meio_poroso)) %>%
mutate(concat_coord = paste0(as.character(x),as.character(y),as.character(z))) %>%
as.data.frame())
# creating look back for time series prediction
look_back = function(df, colNames, lags = 1:6){ #data until one hour back
n = nrow(df)
vet_names = colNames
for(lag in lags){
for(col in vet_names){
new_col = paste0(col,"_prev_",lag)
row_range_in = 1:(n-lag)
df[,new_col] = c(rep(NA,lag), unname(df[row_range_in,col]))
}
}
return(df)
}
#Function to apply look back
apply_lback = function(df){
#criando campo concatenado para filtro
unique_coords = unique(df$concat_coord)
dfi = list()
#defining columns to be lagged
col_lagged = df %>%
select(-c(data,hora,medicao, x, y, z, cenario, concat_coord)) %>%
colnames()
# for filtering and lagging for each coordinate
for(xyz in unique_coords){
dfi[[xyz]] = df %>% filter(concat_coord == xyz)
dfi[[xyz]] = look_back(dfi[[xyz]], col_lagged)
}
df = do.call(bind_rows, dfi)
sortedNames = mixedsort(colnames(df))
df = df[c(sortedNames)]
save(df, file = paste0("../../data/df",min(df$data),".RData"))
return(df)
}
a = lapply(df_list, function(x) apply_lback(x))
head(a)
library(tidyverse)
library(gtools) #for ordering names
rm(list = ls())
gc()
#Verifying all files to be modified
files_tbmod = list.files('../../data_raw/',
full.names = T)
# Reading_data ------------------------------------------------------------
#reading and storing all files from files_tbmod
#cleaning columns and creatig concat coordinates for future filtering
df_list = lapply(files_tbmod,
function(x) read_delim(x, delim = ",",col_names = TRUE) %>%
select(-X1) %>%
select(-(Lateral_plastico:Meio_poroso)) %>%
mutate(concat_coord = paste0(as.character(x),as.character(y),as.character(z))) %>%
as.data.frame())
# creating look back for time series prediction
look_back = function(df, colNames, lags = 1:6){ #data until one hour back
n = nrow(df)
vet_names = colNames
for(lag in lags){
for(col in vet_names){
new_col = paste0(col,"_prev_",lag)
row_range_in = 1:(n-lag)
df[,new_col] = c(rep(NA,lag), unname(df[row_range_in,col]))
}
}
return(df)
}
#Function to apply look back
apply_lback = function(df){
#criando campo concatenado para filtro
unique_coords = unique(df$concat_coord)
dfi = list()
#defining columns to be lagged
col_lagged = df %>%
select(-c(data,hora,medicao, x, y, z, cenario, concat_coord, range_datas)) %>%
colnames()
# for filtering and lagging for each coordinate
for(xyz in unique_coords){
dfi[[xyz]] = df %>% filter(concat_coord == xyz)
dfi[[xyz]] = look_back(dfi[[xyz]], col_lagged)
}
df = do.call(bind_rows, dfi)
sortedNames = mixedsort(colnames(df))
df = df[c(sortedNames)]
save(df, file = paste0("../../data/df",min(df$data),".RData"))
return(df)
}
a = lapply(df_list, function(x) apply_lback(x))
head(a[[1]])
setwd("C:/Users/vinic/Google Drive/Mestrado/pratical_project/variability_part2/greenhouse_tseries/code")
files_tbmodel = list.files('../../data/',
full.names = T)
files_tbmodel
files_tbmodel = list.files('../../data/',
full.names = T)
x = files_tbmodel[1]
dat = get(load(x))
head(dat)
#para ordenar os dados por data e hora
dat = dat %>% arrange(data,hora)
head(dat)
unique(dat$data)
which(dat$data %in% unique(dat$data[1:7]))
library(mlr)
?makeResampleDesc
rmod = makeResampleDesc(“FixedWindowCV”, iters = 10, predict='both')
rmod = makeResampleDesc('FixedWindowCV', iters = 10, predict='both')
rmod = makeResampleDesc('FixedWindowCV', predict='both')
rmod
?rep
rep(1:10, each = 2)
#creating blocking column for CV - train_lines(1:10) and test_lines(11)
blocking_train = rep(1:10, each = length(train_lines)/10)
#train_lines are those considering the 7 first dates. Test is all the three remaining days
train_lines = which(dat$data %in% unique(dat$data[1:7]))
#creating blocking column for CV - train_lines(1:10) and test_lines(11)
blocking_train = rep(1:10, each = length(train_lines)/10)
blocking_train
blocking_test = rep(11, length(test_lines))
test_lines = which(dat$data %in% unique(dat$data[8:10]))
blocking_test = rep(11, length(test_lines))
blocking_test
length(train_lines)
length(test_lines)
unique(dat$data[8:10]))
unique(dat$data[8:10])
dat$data %in% unique(dat$data[8:10])
unique(dat$data)[8:10]
which(dat$data %in% unique(dat$data)[8:10])
dim(dat)
train_lines = which(dat$data %in% unique(dat$data)[1:7])
test_lines = which(dat$data %in% unique(dat$data)[8:10])
length(train_lines)
length(test_lines)
head(dat)
#Tirando as colunas que afetarao a modelagem. Ou possuem correlacao ou nao fazem parte do set up
dat = dat %>% select(-c(data,hora,medicao, concat_coord, cenario))
str(dat)
?mutate_at
dat = dat %>% mutate_at(vars("x","y","z"), funs(as.factor))
?mutate_at
str(dat)
dat = dat[complete.cases(dat),]
dim(dat)
head(dat)
dat = get(load(x))
#para ordenar os dados por data e hora
dat = dat %>% arrange(data,hora)
#train_lines are those considering the 7 first dates. Test is all the three remaining days
train_lines = which(dat$data %in% unique(dat$data)[1:7])
test_lines = which(dat$data %in% unique(dat$data)[8:10])
#creating blocking column for CV - train_lines(1:10) and test_lines(11)
blocking_train = rep(1:10, each = length(train_lines)/10)
blocking_train = blocking_train[1:length(train_lines)]
blocking_test = rep(11, length(test_lines))
block = as.factor(c(blocking_train,blocking_test))
#Tirando as colunas que afetarao a modelagem. Ou possuem correlacao ou nao fazem parte do set up
dat = dat %>% select(-c(data,hora,medicao, concat_coord, cenario))
dat = dat %>% mutate_at(vars("x","y","z"), funs(as.factor))
dat = dat[complete.cases(dat),]
regr_task = makeRegrTask(id = 'brt', data = dat, target = 'temp', blocking = block)
# especifica seed para particionar o conjunto de dados
set.seed(1)
rval = makeFixedHoldoutInstance(train.inds = train_lines, test.inds = test_lines,
size = nrow(dat))
rmod = makeResampleDesc('CV', iters = 10, predict='both', fixed = TRUE)
# especifica que o K deve ser variado de 1 a 20
parameters = makeParamSet(
makeDiscreteParam("n.trees", seq(100,1000, by=100)),
makeDiscreteParam("interaction.depth",seq(1,5,by=1)),
makeNumericParam("shrinkage",lower = 0.001, upper = 0.2)
)
# especifica que usaremos uma busca aleatoria
ctrl = makeTuneControlRandom(maxit = 5L)
parallelStart(mode = 'multicore', cpus = 1, level = 'mlr.tuneParams')
# cria um learner de regressao com gbm, que faz o preprocessing de criar variaveis dummy
base_learner = makeDummyFeaturesWrapper("regr.gbm")
# considera agora que o learner vai ser o melhor resultado de um procedimento de tunning com CV
lrn = makeTuneWrapper(base_learner, resampling = rmod, par.set = parameters, control = ctrl,
measures = mae)
# avalia o modelo resultante do tunning com cross-validation (rmod) usando o conjunto de validacao (rval)
r = resample(lrn, regr_task, resampling = rval, extract = getTuneResult, show.info = TRUE,
models=TRUE, measures = mae)
parallelStop()
dat = get(load(x))
#para ordenar os dados por data e hora
dat = dat %>% arrange(data,hora)
#train_lines are those considering the 7 first dates. Test is all the three remaining days
train_lines = which(dat$data %in% unique(dat$data)[1:7])
test_lines = which(dat$data %in% unique(dat$data)[8:10])
#creating blocking column for CV - train_lines(1:10) and test_lines(11)
blocking_train = rep(1:10, each = length(train_lines)/10)
blocking_train = blocking_train[1:length(train_lines)]
blocking_test = rep(11, length(test_lines))
block = as.factor(c(blocking_train,blocking_test))
dat$block = block
#Tirando as colunas que afetarao a modelagem. Ou possuem correlacao ou nao fazem parte do set up
dat = dat %>% select(-c(data,hora,medicao, concat_coord, cenario))
dat = dat %>% mutate_at(vars("x","y","z"), funs(as.factor))
dat = dat[complete.cases(dat),]
block = dat$block
dat = dat %>% select(-c(block))
regr_task = makeRegrTask(id = 'brt', data = dat, target = 'temp', blocking = block)
# especifica seed para particionar o conjunto de dados
set.seed(1)
rval = makeFixedHoldoutInstance(train.inds = train_lines, test.inds = test_lines,
size = nrow(dat))
rmod = makeResampleDesc('CV', iters = 10, predict='both', fixed = TRUE)
# especifica que o K deve ser variado de 1 a 20
parameters = makeParamSet(
makeDiscreteParam("n.trees", seq(100,1000, by=100)),
makeDiscreteParam("interaction.depth",seq(1,5,by=1)),
makeNumericParam("shrinkage",lower = 0.001, upper = 0.2)
)
# especifica que usaremos uma busca aleatoria
ctrl = makeTuneControlRandom(maxit = 5L)
parallelStart(mode = 'multicore', cpus = 1, level = 'mlr.tuneParams')
# cria um learner de regressao com gbm, que faz o preprocessing de criar variaveis dummy
base_learner = makeDummyFeaturesWrapper("regr.gbm")
# considera agora que o learner vai ser o melhor resultado de um procedimento de tunning com CV
lrn = makeTuneWrapper(base_learner, resampling = rmod, par.set = parameters, control = ctrl,
measures = mae)
# avalia o modelo resultante do tunning com cross-validation (rmod) usando o conjunto de validacao (rval)
r = resample(lrn, regr_task, resampling = rval, extract = getTuneResult, show.info = TRUE,
models=TRUE, measures = mae)
parallelStop()
rmod = makeResampleDesc('CV', predict='both', fixed = TRUE)
rmod = makeResampleDesc('CV', predict='both', fixed = TRUE)
?makeResampleDesc
devtools::install_github("mlr-org/mlr")
install.packages("devtools")
devtools::install_github("mlr-org/mlr")
update.packages("mlr")
sessionInfo()
?makeResampleDesc
devtools::install_github("mlr-org/mlr")
?makeResampleDesc
