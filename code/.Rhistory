vet_names = colNames
for(lag in lags){
for(col in vet_names){
new_col = paste0(col,lag)
row_range_in = 1:(n-lag)
df[,new_col] = c(rep(NA,lag), unname(df[row_range_in,col]))
}
}
return(df)
}
#Function to apply look back
apply_lback = function(df){
#criando campo concatenado para filtro
unique_coords = unique(df$concat_coord)
dfi = list()
#defining columns to be lagged
col_lagged = df %>%
select(-c(data,hora,medicao, x, y, z, cenario, concat_coord)) %>%
colnames()
col_lagged = c("temp","ur")
# for filtering and lagging for each coordinate
for(xyz in unique_coords){
dfi[[xyz]] = df %>% filter(concat_coord == xyz)
dfi[[xyz]] = look_back(dfi[[xyz]], col_lagged)
}
df = do.call(bind_rows, dfi)
sortedNames = mixedsort(colnames(df))
df = df[c(sortedNames)]
save(df, file = paste0("../../data/df",min(df$data),".RData"))
return(df)
}
a = sapply(df_list, function(x) apply_lback(x))
head(a[[1]])
head(a)
a = lapply(df_list, function(x) apply_lback(x))
a[[1]]
library(tidyverse)
library(gtools) #for ordering names
rm(list = ls())
gc()
#Verifying all files to be modified
files_tbmod = list.files('../../data_raw/',
full.names = T)
# Reading_data ------------------------------------------------------------
#reading and storing all files from files_tbmod
#cleaning columns and creatig concat coordinates for future filtering
df_list = lapply(files_tbmod,
function(x) read_delim(x, delim = ",",col_names = TRUE) %>%
select(-X1) %>%
select(-(Lateral_plastico:Meio_poroso)) %>%
mutate(concat_coord = paste0(as.character(x),as.character(y),as.character(z))) %>%
as.data.frame())
# creating look back for time series prediction
look_back = function(df, colNames, lags = 1:6){ #data until one hour back
n = nrow(df)
vet_names = colNames
for(lag in lags){
for(col in vet_names){
new_col = paste0(col,lag)
row_range_in = 1:(n-lag)
df[,new_col] = c(rep(NA,lag), unname(df[row_range_in,col]))
}
}
return(df)
}
#Function to apply look back
apply_lback = function(df){
#criando campo concatenado para filtro
unique_coords = unique(df$concat_coord)
dfi = list()
#defining columns to be lagged
col_lagged = df %>%
select(-c(data,hora,medicao, x, y, z, cenario, concat_coord)) %>%
colnames()
# for filtering and lagging for each coordinate
for(xyz in unique_coords){
dfi[[xyz]] = df %>% filter(concat_coord == xyz)
dfi[[xyz]] = look_back(dfi[[xyz]], col_lagged)
}
df = do.call(bind_rows, dfi)
sortedNames = mixedsort(colnames(df))
df = df[c(sortedNames)]
save(df, file = paste0("../../data/df",min(df$data),".RData"))
return(df)
}
a = lapply(df_list, function(x) apply_lback(x))
a[[1]]
library(tidyverse)
library(gtools) #for ordering names
rm(list = ls())
gc()
#Verifying all files to be modified
files_tbmod = list.files('../../data_raw/',
full.names = T)
# Reading_data ------------------------------------------------------------
#reading and storing all files from files_tbmod
#cleaning columns and creatig concat coordinates for future filtering
df_list = lapply(files_tbmod,
function(x) read_delim(x, delim = ",",col_names = TRUE) %>%
select(-X1) %>%
select(-(Lateral_plastico:Meio_poroso)) %>%
mutate(concat_coord = paste0(as.character(x),as.character(y),as.character(z))) %>%
as.data.frame())
# creating look back for time series prediction
look_back = function(df, colNames, lags = 1:6){ #data until one hour back
n = nrow(df)
vet_names = colNames
for(lag in lags){
for(col in vet_names){
new_col = paste0("prev_",lag,"_",col)
row_range_in = 1:(n-lag)
df[,new_col] = c(rep(NA,lag), unname(df[row_range_in,col]))
}
}
return(df)
}
#Function to apply look back
apply_lback = function(df){
#criando campo concatenado para filtro
unique_coords = unique(df$concat_coord)
dfi = list()
#defining columns to be lagged
col_lagged = df %>%
select(-c(data,hora,medicao, x, y, z, cenario, concat_coord)) %>%
colnames()
# for filtering and lagging for each coordinate
for(xyz in unique_coords){
dfi[[xyz]] = df %>% filter(concat_coord == xyz)
dfi[[xyz]] = look_back(dfi[[xyz]], col_lagged)
}
df = do.call(bind_rows, dfi)
sortedNames = mixedsort(colnames(df))
df = df[c(sortedNames)]
save(df, file = paste0("../../data/df",min(df$data),".RData"))
return(df)
}
a = lapply(df_list, function(x) apply_lback(x))
head(a[[2]])
library(tidyverse)
library(gtools) #for ordering names
rm(list = ls())
gc()
#Verifying all files to be modified
files_tbmod = list.files('../../data_raw/',
full.names = T)
# Reading_data ------------------------------------------------------------
#reading and storing all files from files_tbmod
#cleaning columns and creatig concat coordinates for future filtering
df_list = lapply(files_tbmod,
function(x) read_delim(x, delim = ",",col_names = TRUE) %>%
select(-X1) %>%
select(-(Lateral_plastico:Meio_poroso)) %>%
mutate(concat_coord = paste0(as.character(x),as.character(y),as.character(z))) %>%
as.data.frame())
# creating look back for time series prediction
look_back = function(df, colNames, lags = 1:6){ #data until one hour back
n = nrow(df)
vet_names = colNames
for(lag in lags){
for(col in vet_names){
new_col = paste0(col,"_prev_",lag)
row_range_in = 1:(n-lag)
df[,new_col] = c(rep(NA,lag), unname(df[row_range_in,col]))
}
}
return(df)
}
#Function to apply look back
apply_lback = function(df){
#criando campo concatenado para filtro
unique_coords = unique(df$concat_coord)
dfi = list()
#defining columns to be lagged
col_lagged = df %>%
select(-c(data,hora,medicao, x, y, z, cenario, concat_coord)) %>%
colnames()
# for filtering and lagging for each coordinate
for(xyz in unique_coords){
dfi[[xyz]] = df %>% filter(concat_coord == xyz)
dfi[[xyz]] = look_back(dfi[[xyz]], col_lagged)
}
df = do.call(bind_rows, dfi)
sortedNames = mixedsort(colnames(df))
df = df[c(sortedNames)]
save(df, file = paste0("../../data/df",min(df$data),".RData"))
return(df)
}
a = lapply(df_list, function(x) apply_lback(x))
head(a)
library(tidyverse)
library(gtools) #for ordering names
rm(list = ls())
gc()
#Verifying all files to be modified
files_tbmod = list.files('../../data_raw/',
full.names = T)
# Reading_data ------------------------------------------------------------
#reading and storing all files from files_tbmod
#cleaning columns and creatig concat coordinates for future filtering
df_list = lapply(files_tbmod,
function(x) read_delim(x, delim = ",",col_names = TRUE) %>%
select(-X1) %>%
select(-(Lateral_plastico:Meio_poroso)) %>%
mutate(concat_coord = paste0(as.character(x),as.character(y),as.character(z))) %>%
as.data.frame())
# creating look back for time series prediction
look_back = function(df, colNames, lags = 1:6){ #data until one hour back
n = nrow(df)
vet_names = colNames
for(lag in lags){
for(col in vet_names){
new_col = paste0(col,"_prev_",lag)
row_range_in = 1:(n-lag)
df[,new_col] = c(rep(NA,lag), unname(df[row_range_in,col]))
}
}
return(df)
}
#Function to apply look back
apply_lback = function(df){
#criando campo concatenado para filtro
unique_coords = unique(df$concat_coord)
dfi = list()
#defining columns to be lagged
col_lagged = df %>%
select(-c(data,hora,medicao, x, y, z, cenario, concat_coord, range_datas)) %>%
colnames()
# for filtering and lagging for each coordinate
for(xyz in unique_coords){
dfi[[xyz]] = df %>% filter(concat_coord == xyz)
dfi[[xyz]] = look_back(dfi[[xyz]], col_lagged)
}
df = do.call(bind_rows, dfi)
sortedNames = mixedsort(colnames(df))
df = df[c(sortedNames)]
save(df, file = paste0("../../data/df",min(df$data),".RData"))
return(df)
}
a = lapply(df_list, function(x) apply_lback(x))
head(a[[1]])
setwd("C:/Users/vinic/Google Drive/Mestrado/pratical_project/variability_part2/greenhouse_tseries/code")
files_tbmodel = list.files('../../data/',
full.names = T)
files_tbmodel
files_tbmodel = list.files('../../data/',
full.names = T)
x = files_tbmodel[1]
dat = get(load(x))
head(dat)
#para ordenar os dados por data e hora
dat = dat %>% arrange(data,hora)
head(dat)
unique(dat$data)
which(dat$data %in% unique(dat$data[1:7]))
library(mlr)
?makeResampleDesc
rmod = makeResampleDesc(“FixedWindowCV”, iters = 10, predict='both')
rmod = makeResampleDesc('FixedWindowCV', iters = 10, predict='both')
rmod = makeResampleDesc('FixedWindowCV', predict='both')
rmod
?rep
rep(1:10, each = 2)
#creating blocking column for CV - train_lines(1:10) and test_lines(11)
blocking_train = rep(1:10, each = length(train_lines)/10)
#train_lines are those considering the 7 first dates. Test is all the three remaining days
train_lines = which(dat$data %in% unique(dat$data[1:7]))
#creating blocking column for CV - train_lines(1:10) and test_lines(11)
blocking_train = rep(1:10, each = length(train_lines)/10)
blocking_train
blocking_test = rep(11, length(test_lines))
test_lines = which(dat$data %in% unique(dat$data[8:10]))
blocking_test = rep(11, length(test_lines))
blocking_test
length(train_lines)
length(test_lines)
unique(dat$data[8:10]))
unique(dat$data[8:10])
dat$data %in% unique(dat$data[8:10])
unique(dat$data)[8:10]
which(dat$data %in% unique(dat$data)[8:10])
dim(dat)
train_lines = which(dat$data %in% unique(dat$data)[1:7])
test_lines = which(dat$data %in% unique(dat$data)[8:10])
length(train_lines)
length(test_lines)
head(dat)
#Tirando as colunas que afetarao a modelagem. Ou possuem correlacao ou nao fazem parte do set up
dat = dat %>% select(-c(data,hora,medicao, concat_coord, cenario))
str(dat)
?mutate_at
dat = dat %>% mutate_at(vars("x","y","z"), funs(as.factor))
?mutate_at
str(dat)
dat = dat[complete.cases(dat),]
dim(dat)
head(dat)
dat = get(load(x))
#para ordenar os dados por data e hora
dat = dat %>% arrange(data,hora)
#train_lines are those considering the 7 first dates. Test is all the three remaining days
train_lines = which(dat$data %in% unique(dat$data)[1:7])
test_lines = which(dat$data %in% unique(dat$data)[8:10])
#creating blocking column for CV - train_lines(1:10) and test_lines(11)
blocking_train = rep(1:10, each = length(train_lines)/10)
blocking_train = blocking_train[1:length(train_lines)]
blocking_test = rep(11, length(test_lines))
block = as.factor(c(blocking_train,blocking_test))
#Tirando as colunas que afetarao a modelagem. Ou possuem correlacao ou nao fazem parte do set up
dat = dat %>% select(-c(data,hora,medicao, concat_coord, cenario))
dat = dat %>% mutate_at(vars("x","y","z"), funs(as.factor))
dat = dat[complete.cases(dat),]
regr_task = makeRegrTask(id = 'brt', data = dat, target = 'temp', blocking = block)
# especifica seed para particionar o conjunto de dados
set.seed(1)
rval = makeFixedHoldoutInstance(train.inds = train_lines, test.inds = test_lines,
size = nrow(dat))
rmod = makeResampleDesc('CV', iters = 10, predict='both', fixed = TRUE)
# especifica que o K deve ser variado de 1 a 20
parameters = makeParamSet(
makeDiscreteParam("n.trees", seq(100,1000, by=100)),
makeDiscreteParam("interaction.depth",seq(1,5,by=1)),
makeNumericParam("shrinkage",lower = 0.001, upper = 0.2)
)
# especifica que usaremos uma busca aleatoria
ctrl = makeTuneControlRandom(maxit = 5L)
parallelStart(mode = 'multicore', cpus = 1, level = 'mlr.tuneParams')
# cria um learner de regressao com gbm, que faz o preprocessing de criar variaveis dummy
base_learner = makeDummyFeaturesWrapper("regr.gbm")
# considera agora que o learner vai ser o melhor resultado de um procedimento de tunning com CV
lrn = makeTuneWrapper(base_learner, resampling = rmod, par.set = parameters, control = ctrl,
measures = mae)
# avalia o modelo resultante do tunning com cross-validation (rmod) usando o conjunto de validacao (rval)
r = resample(lrn, regr_task, resampling = rval, extract = getTuneResult, show.info = TRUE,
models=TRUE, measures = mae)
parallelStop()
dat = get(load(x))
#para ordenar os dados por data e hora
dat = dat %>% arrange(data,hora)
#train_lines are those considering the 7 first dates. Test is all the three remaining days
train_lines = which(dat$data %in% unique(dat$data)[1:7])
test_lines = which(dat$data %in% unique(dat$data)[8:10])
#creating blocking column for CV - train_lines(1:10) and test_lines(11)
blocking_train = rep(1:10, each = length(train_lines)/10)
blocking_train = blocking_train[1:length(train_lines)]
blocking_test = rep(11, length(test_lines))
block = as.factor(c(blocking_train,blocking_test))
dat$block = block
#Tirando as colunas que afetarao a modelagem. Ou possuem correlacao ou nao fazem parte do set up
dat = dat %>% select(-c(data,hora,medicao, concat_coord, cenario))
dat = dat %>% mutate_at(vars("x","y","z"), funs(as.factor))
dat = dat[complete.cases(dat),]
block = dat$block
dat = dat %>% select(-c(block))
regr_task = makeRegrTask(id = 'brt', data = dat, target = 'temp', blocking = block)
# especifica seed para particionar o conjunto de dados
set.seed(1)
rval = makeFixedHoldoutInstance(train.inds = train_lines, test.inds = test_lines,
size = nrow(dat))
rmod = makeResampleDesc('CV', iters = 10, predict='both', fixed = TRUE)
# especifica que o K deve ser variado de 1 a 20
parameters = makeParamSet(
makeDiscreteParam("n.trees", seq(100,1000, by=100)),
makeDiscreteParam("interaction.depth",seq(1,5,by=1)),
makeNumericParam("shrinkage",lower = 0.001, upper = 0.2)
)
# especifica que usaremos uma busca aleatoria
ctrl = makeTuneControlRandom(maxit = 5L)
parallelStart(mode = 'multicore', cpus = 1, level = 'mlr.tuneParams')
# cria um learner de regressao com gbm, que faz o preprocessing de criar variaveis dummy
base_learner = makeDummyFeaturesWrapper("regr.gbm")
# considera agora que o learner vai ser o melhor resultado de um procedimento de tunning com CV
lrn = makeTuneWrapper(base_learner, resampling = rmod, par.set = parameters, control = ctrl,
measures = mae)
# avalia o modelo resultante do tunning com cross-validation (rmod) usando o conjunto de validacao (rval)
r = resample(lrn, regr_task, resampling = rval, extract = getTuneResult, show.info = TRUE,
models=TRUE, measures = mae)
parallelStop()
rmod = makeResampleDesc('CV', predict='both', fixed = TRUE)
rmod = makeResampleDesc('CV', predict='both', fixed = TRUE)
?makeResampleDesc
devtools::install_github("mlr-org/mlr")
install.packages("devtools")
devtools::install_github("mlr-org/mlr")
update.packages("mlr")
sessionInfo()
?makeResampleDesc
devtools::install_github("mlr-org/mlr")
?makeResampleDesc
setwd("C:/Users/BRVAVL/OneDrive - C&A Modas Ltda/github_vavlopes/greenhouse_tseries/code")
library(tidyverse)
library(gtools) #for ordering names
library(reticulate)
rm(list = ls())
gc()
# Saving files into pickle format -----------------------------------------
py_save_object <- function(object, filename, pickle = "pickle") {
builtins <- import_builtins()
pickle <- import(pickle)
handle <- builtins$open(filename, "wb")
on.exit(handle$close(), add = TRUE)
pickle$dump(object, handle, protocol = pickle$HIGHEST_PROTOCOL)
}
#Verifying all files to be modified
files_tbmod = list.files('../../data_raw/',
full.names = T)
# Reading_data ------------------------------------------------------------
#reading and storing all files from files_tbmod
#cleaning columns and creatig concat coordinates for future filtering
df_list = lapply(files_tbmod,
function(x) read_delim(x, delim = ",",col_names = TRUE) %>%
select(-X1) %>%
select(-(Lateral_plastico:Meio_poroso)) %>%
mutate(concat_coord = paste0(as.character(x),as.character(y),as.character(z))) %>%
as.data.frame())
# creating look back for time series prediction
look_back = function(df, colNames, lags = 1:6){ #data until one hour back
n = nrow(df)
vet_names = colNames
for(lag in lags){
for(col in vet_names){
new_col = paste0(col,"_prev_",lag)
row_range_in = 1:(n-lag)
df[,new_col] = c(rep(NA,lag), unname(df[row_range_in,col]))
}
}
return(df)
}
#Function to apply look back
apply_lback = function(df){
#criando campo concatenado para filtro
unique_coords = unique(df$concat_coord)
dfi = list()
#defining columns to be lagged
col_lagged = df %>%
select(-c(data,hora,medicao, x, y, z, cenario, concat_coord, range_datas)) %>%
colnames()
# for filtering and lagging for each coordinate (on columns previously selected)
for(xyz in unique_coords){
dfi[[xyz]] = df %>% filter(concat_coord == xyz)
dfi[[xyz]] = look_back(dfi[[xyz]], col_lagged)
}
df = do.call(bind_rows, dfi)
sortedNames = mixedsort(colnames(df))
df = df[c(sortedNames)]
save(df, file = paste0("../../data/df",min(df$data),".RData"))
py_save_object(df, paste0("../../data/df",min(df$data),".pickle"), pickle = "pickle")
return(df)
}
a = lapply(df_list, function(x) apply_lback(x))
library(tidyverse)
library(gtools) #for ordering names
library(reticulate)
rm(list = ls())
gc()
# Saving files into pickle format -----------------------------------------
py_save_object <- function(object, filename, pickle = "pickle") {
builtins <- import_builtins()
pickle <- import(pickle)
handle <- builtins$open(filename, "wb")
on.exit(handle$close(), add = TRUE)
pickle$dump(object, handle, protocol = pickle$HIGHEST_PROTOCOL)
}
#Verifying all files to be modified
files_tbmod = list.files('../../data_raw/',
full.names = T)
# Reading_data ------------------------------------------------------------
#reading and storing all files from files_tbmod
#cleaning columns and creatig concat coordinates for future filtering
df_list = lapply(files_tbmod,
function(x) read_delim(x, delim = ",",col_names = TRUE,trim_ws = T) %>%
select(-X1) %>%
select(-(Lateral_plastico:Meio_poroso)) %>%
mutate(concat_coord = paste0(as.character(x),as.character(y),as.character(z))) %>%
as.data.frame())
# creating look back for time series prediction
look_back = function(df, colNames, lags = 1:6){ #data until one hour back
n = nrow(df)
vet_names = colNames
for(lag in lags){
for(col in vet_names){
new_col = paste0(col,"_prev_",lag)
row_range_in = 1:(n-lag)
df[,new_col] = c(rep(NA,lag), unname(df[row_range_in,col]))
}
}
return(df)
}
#Function to apply look back
apply_lback = function(df){
#criando campo concatenado para filtro
unique_coords = unique(df$concat_coord)
dfi = list()
#defining columns to be lagged
col_lagged = df %>%
select(-c(data,hora,medicao, x, y, z, cenario, concat_coord, range_datas)) %>%
colnames()
# for filtering and lagging for each coordinate (on columns previously selected)
for(xyz in unique_coords){
dfi[[xyz]] = df %>% filter(concat_coord == xyz)
dfi[[xyz]] = look_back(dfi[[xyz]], col_lagged)
}
df = do.call(bind_rows, dfi)
sortedNames = mixedsort(colnames(df))
df = df[c(sortedNames)]
save(df, file = paste0("../../data/df",min(df$data),".RData"))
py_save_object(df, paste0("../../data/df",min(df$data),".pickle"), pickle = "pickle")
return(df)
}
a = lapply(df_list, function(x) apply_lback(x))
str(df)
a[[1]]
str(a[[1]])
